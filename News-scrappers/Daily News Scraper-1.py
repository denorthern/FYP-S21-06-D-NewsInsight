# -*- coding: utf-8 -*-
"""Final_Newextraction_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-jFtjBHVGl86KXyrGATb7E8oT1Tvc_Ee

# **News Scrapper**
News Sources: TOI - Dawn - Tribune - NYT
"""

import os
import sys
import time
import requests
import threading
import collections
import numpy as np

from datetime import datetime
from datetime import date 
from datetime import timedelta
from bs4 import BeautifulSoup

from urllib.error import HTTPError
import newspaper
from newspaper import Article
def daterange(start, end):
      for n in range(int ((end-start).days)):
        yield start+timedelta(n)  
"""### Scrapper parent class: NewsExtractor



"""

class NewsExtractor:
  articles=[]
  dir=""

  def setdirpath(self, path):
      # Check path exists
      Existflag = os.path.exists(path)
      if not Existflag:
      # Creating new directory as it not exists 
        os.makedirs(path)
      self.dir=path
      path=path+"-byDate"
      # Check path exists
      Existflag = os.path.exists(path)
      if not Existflag:
      # Creating new directory as it not exists 
        os.makedirs(path)
  def downloadarticles(self, i, array1,d, m, year):   
      le=len(array1)
      print(" threade:  => ", i)
      c=0
      for q in array1:
          print(" threade:", i, "  => (",  c,"/",le,")")
          x=q
          (s, l, De, Dp, t, aut, art)=self.getarticle(x, d, m, year)
           
          if (s+l+De+Dp+t+aut+art!="0"+"0"+"0"+"0"+"0"+"0"+"0"):
                  non=['/',':','*','?','"','<','>','|','\n', '\\', '\r-']
                  name=s+" _ "+Dp+" _ "+t+".txt"
                  for n in non:
                      name=name.replace(n, '-')
                  
                  f = open(self.dir+"/"+name, "w", encoding="utf-8")
                  f.write("Source: " + s)
                  f.write("\nLink: " + l)
                  f.write("\nDateOfExtraction: " + De)
                  f.write("\nDateOfPublication: " + Dp)
                  f.write("\nTitle: " + t)
                  f.write("\nAuthor: " + aut)
                  f.write("\nArticle: " + art)
                  f.close()
                  name=t+".txt"
                  for n in non:
                      name=name.replace(n, '-')
                      
                  path=self.dir+"-byDate"+"/"+Dp
                  # Check path exists
                  Existflag = os.path.exists(path)
                  if not Existflag:
                  # Creating new directory as it not exists 
                    os.makedirs(path)        
                  f = open(path+"/"+name, "w", encoding="utf-8")
                  f.write("Source: " + s)
                  f.write("\nLink: " + l)
                  f.write("\nDateOfExtraction: " + De)
                  f.write("\nDateOfPublication: " + Dp)
                  f.write("\nTitle: " + t)
                  f.write("\nAuthor: " + aut)
                  f.write("\nArticle: " + art)
                  f.close()
                  
                  NewlyAddedFiles = "F:/xampp1/htdocs/extraction/urlrecords/NewlyAddedFilesUrl/"+Dp+"/"+self.source+".txt"
                  file2 = open(NewlyAddedFiles,"a+")#append mode
                  file2.write(path+"/"+name+" \n") 
                  file2.close()
          c=c+1
      print(" => download complete! thread: ", i)
  
  def downloaddata(self, d, m, year, url, chk ):

      if chk==0:
        self.getdatelinks(d, m, year)
      else:
        self.getdatelinks1(url)

      self.articles = list(dict.fromkeys(self.articles))
      # -------------------------------
      
      day=int(d)
      month=int(m)
      m2=str(month)
      d2=str(day)
      if month<10:
        m2="0"+str(month)
        
      d2=day
      if day<10:
        d2="0"+str(day)
      
      
      day=day-1
      if day==0:
          if(month==3):
              day=28
          if month in [4,5,7,8,10,12]:
              day=30
          if month in [1,2,4,6,9,11]: 
              day=31
          month=month-1
          if(month==0):
              month=12
      m1=month
      if month<10:
        m1="0"+str(month)
        
      d1=day
      if day<10:
        d1="0"+str(day)
      
      old1=[]
      path = "F:/xampp1/htdocs/extraction/urlrecords/"+str(year)+"-"+str(m1)+"-"+str(d1)
      filename=path+"/"+self.source+".txt"
      # Check path exists
      Existflag = os.path.exists(path)
      if not Existflag:
        pass
      else:
        file1 = open(filename,"r+")
        old1=file1.readlines()     
        file1.close()
        
      path = "F:/xampp1/htdocs/extraction/urlrecords/"+str(year)+"-"+str(m2)+"-"+str(d2)
      # Check path exists
      Existflag = os.path.exists(path)
      if not Existflag:
      # Creating new directory as it not exists 
        os.makedirs(path)
      filename=path+"/"+self.source+".txt"
      file1 = open(filename ,"a+")#append mode
      file1.close()

      file1 = open(filename,"r+")
      old=file1.readlines()      
      temp=[]
      file1 = open(filename,"a+")#append mode
      for item in self.articles:
        if (item+" \n") in old or (item+" \n") in old1:
          self.articles.remove(item)
        else:
          temp.append(item)
          file1.write(item+" \n") 
          
      file1.close()
      self.articles=temp
      
      #--------------------Creating Newly Added url file------------#
      path = "F:/xampp1/htdocs/extraction/urlrecords/NewlyAddedFilesUrl/"+str(year)+"-"+str(m2)+"-"+str(d2)
      Existflag = os.path.exists(path)
      if not Existflag:
        os.makedirs(path)
      filename=path+"/"+self.source+".txt"
      file1 = open(filename,"w")
      file1.close()
      # ------------------------------------------------------------#

      #print(len( articles))
      print(" => links extracted :" + str(len(self.articles)))
      arr = np.array(self.articles)
      splitno=1
      if len( self.articles) > 1000:
          splitno=14 #change according to number of articles must be less then no of articles 
      if len( self.articles) > 500 and len( self.articles) <= 1000:
          splitno=11 #change according to number of articles must be less then no of articles 
      if len( self.articles) > 100 and len( self.articles) <= 500:
          splitno=8 #change according to number of articles must be less then no of articles  
      if len( self.articles) >20 and len( self.articles) <= 100:
          splitno=4
      if len( self.articles) <20:
          splitno=1  
      newarr = np.array_split(arr, splitno)
      i=0
      while(i<splitno):
          a=newarr[i]
          t1 = threading.Thread(target=self.downloadarticles, args=(i,a,d, m, year,  ))
          t1.start()
          time.sleep(8)
          i=i+1
      t1.join()  

  def daily(self):
      print(" => Daily news article parser")
      Datetoday=str(datetime.date(datetime.now()))
      today=Datetoday.split('-',2)
      year=int(today[0])
      month=int(today[1])
      m2=month
      if month<10:
        m2="0"+str(month)
        
      day=int(today[2])
      d2=day
      if day<10:
        d2="0"+str(day)
      
      
      day=day-1
      if day==0:
          if(month==3):
              day=28
          if month in [4,5,7,8,10,12]:
              day=30
          if month in [1,2,4,6,9,11]: 
              day=31
          month=month-1
          if(month==0):
              month=12
      m1=month
      if month<10:
        m1="0"+str(month)
        
      d1=day
      if day<10:
        d1="0"+str(day)

      date1="2021-01-01"
      date2="2021-01-01"
      date1=str(year)+"-"+str(m1)+"-"+str(d1)
      date2=str(year)+"-"+str(m2)+"-"+str(d2)
      self.rangedate(date1, date2)

  def weekly(self):
      print(" => weekly news article parser")

      Datetoday=str(datetime.date(datetime.now()))
      today=Datetoday.split('-',2)
      year=int(today[0])
      month=int(today[1])
      day=int(today[2])
      day=day-1
      if day==0:
          if(month==3):
              day=28
          if month in [4,5,7,8,10,12]:
              day=30
          if month in [1,2,4,6,9,11]: 
              day=31
          month=month-1
          if(month==0):
              month=12
      print(" ----", Datetoday, "-----\n")
      #input1 from: 
      m2=month #month
      d2=day #day
      m1=month #month
      d1=day #day

      c=2
      if c==2: 
        d1=d2-7
        if d1<=0:
          if(month==3):
            day=28
          if month in [3,5,7,8,10,12]:
            day=30    
          if month in [1,2,4,6,9,11]: 
            day=31 
          d1=day+d1
          m1=month-1
          if(m1==0):
            month=12
        else:
          m1=m2  
      date1=str(year)+"-"+str(m1)+"-"+str(d1)
      date2=str(year)+"-"+str(m2)+"-"+str(d2)
      self.rangedate(date1, date2)

  def monthly(self):
      print(" => monthly news article parser")
      Datetoday=str(datetime.date(datetime.now()))
      today=Datetoday.split('-',2)
      year=int(today[0])
      month=int(today[1])
      day=int(today[2])
      day=day-1
      if day==0:
          if(month==3):
              day=28
          if month in [4,5,7,8,10,12]:
              day=30       
          if month in [1,2,4,6,9,11]: 
              day=31
          month=month-1
          if(month==0):
              month=12
      #input1 from: 
      m2=month #month
      d2=day #day
      m1=month #month
      d1=day #day

      c=3
      if c==3:  #monthly
        m1=m2-1
        if(m1==0):
          m1=12
      date1=str(year)+"-"+str(m1)+"-"+str(d1)
      date2=str(year)+"-"+str(m2)+"-"+str(d2)
      self.rangedate(date1, date2)

  def getarticle(self, x):
    pass  

  def getdatelinks(self, d, m, year):
    pass 

  def extract(self, d, m, year):
    pass 

  def rangedate(self, date1, date2):
      print(" => specified news article parser")
      c=0
      self.extract(date1, date2, c)

"""### Scrapper: Times Of India News"""

class TOI(NewsExtractor):
  source="toi"
  def getdatelinks(self, d, m, y):
      months = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'] 
      
      day=str(d)
      year=str(y)  
      month=months[m-1]
      print(" => getting article links...........", day,month+",",year)
      import requests
      from bs4 import BeautifulSoup
      catagories=["1"]

      #check="/article/"
      #check1="/opinion/"
      starttime=44269
      startmonth=3
      startday=14
      
      start=date(2021, startmonth, startday)
      end=date(y, m, d)
      difference=0
      for singledate in daterange(start, end):
        #print(singledate.strftime("%Y-%m-%d"))
        difference=difference=+1
      newtime=starttime+difference

      areas=["/world/", "/viral-news/", "/most-searched-products/", "/politics/",  "/sports/", "/entertainment/hindi/", "/entertainment/english/", "/life-style/", "/education/", "/elections/", "/india/", "/business/"]

      for c in catagories:
        #   
        url="https://timesofindia.indiatimes.com/"+year+"/"+month+"/"+str(day)+"/archivelist/year-"+year+",month-"+month+",starttime-"+str(newtime)+".cms"
        print(">>>>>", url)
        reqs = requests.get(url)
        soup = BeautifulSoup(reqs.text, 'html.parser')
        
        for link in soup.find_all("a"):
            if "/articleshow/" in str(link.get('href')):
              url=str(link.get('href'))
              validation=0
              for area in areas:
                if area in url:
                  validation=1
              if  url not in self.articles and validation==1:    
                self.articles.append(url)
                #print(url)    
  def getarticle(self, x, d, m, year):
    print(" => downloading..... "+x)
    flage=1
    try:
      toi_article = Article(x, language="en") # en for English
      toi_article.download()
      toi_article.parse()
    except newspaper.article.ArticleException:
      flage=0
    
    if flage==1:
      #To extract title
      #print("Article's Title:")
      #print(toi_article.title)
      #print("n")
      #To extract text
      #print("Article's Text:")
      #print(toi_article.text)
      #print("n")
      #print(toi_article.publish_date)
      link=x
      title=str(toi_article.title)
      author=str(toi_article.authors)
      article=str(toi_article.text)
      source="Times of india"
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(year)  
      month=months[m-1]
      DateOfPublication=year+"-"+month+"-"+day
      DateOfExtraction=str(datetime.date(datetime.now()))
      #print(source)
      #print(DateOfExtraction)
      #print(DateOfPublication)
      #print(title)
      #print(author)
      #print(article)
      return source, link, DateOfExtraction, DateOfPublication, title, author, article
    else:
      return "0","0","0","0","0","0" ,"0"  
  def extract(self, date1, date2, c):
    from datetime import datetime
    Datetoday=str(datetime.date(datetime.now()))
    print("\n => scraper: times of india news")



    if c==0: #defined range
      print(" to:",date1, "----- from:",date2,"\n")
      datei=date1.split('-',2)
      year=int(datei[0])
      m1=int(datei[1])
      d1=int(datei[2])
      datef=date2.split('-',2)
      year=int(datef[0])
      m2=int(datef[1])
      d2=int(datef[2])

      url=""
      chk=0
      for j in range(m1, m2+1):
          if(j==2):
              df=28
          if j in [4,5,7,8,10,12]:
              df=31       
          if j in [1,2,4,6,9,11]: 
              df=30
          di=1
          if j==m1:
              di=d1
          if j==m2:
              df=d2
          for i in range(di, df+1):
              self.articles.clear()
              self.downloaddata(i, j, year, url, chk)

"""### Scrapper: The New-York Times News"""

class NYT(NewsExtractor):
  source="nyt"
  def getdatelinks(self, d, m, y):
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(y)  
      month=months[m-1]
      print(" => getting article links...........", day,month+",",year)
      import requests
      from bs4 import BeautifulSoup
      catagories=["&sections=World%7Cnyt%3A%2F%2Fsection%2F70e865b6-cc70-5181-84c9-8368b3a5c34b",
      "&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c",
      "&sections=Travel%7Cnyt%3A%2F%2Fsection%2Fb2fb7c08-4f8e-5cff-8e14-aff8a49a9934",
      "&sections=New%20York%7Cnyt%3A%2F%2Fsection%2F39480374-66d3-5603-9ce1-58cfa12988e2",
      "&sections=Business%7Cnyt%3A%2F%2Fsection%2F0415b2b0-513a-5e78-80da-21ab770cb753",
      "&sections=Opinion%7Cnyt%3A%2F%2Fsection%2Fd7a71185-aa60-5635-bce0-5fab76c7c297",
      "&sections=Books%7Cnyt%3A%2F%2Fsection%2F550f75e2-fc37-5d5c-9dd1-c665ac221b49",
      "&sections=Arts%7Cnyt%3A%2F%2Fsection%2F6e6ee292-b4bd-5006-a619-9ceab03524f2",
      "&sections=",
      ""]
      areas=["/world/","/us/", "/sports/", "/style/", "/business/", "/movies/", "/opinion/", "/books/", "/arts/", "/nyregion/", "/briefing/"]
    
      #check="/article/"
      #check1="/opinion/"
      for c in catagories:
        url="https://www.nytimes.com/search?dropmab=false&endDate="+year+month+str(day)+"&query="+c+"&sort=newest&startDate="+year+month+str(day)+"&types=article"
        print(">>>>>", url)
        reqs = requests.get(url)
        soup = BeautifulSoup(reqs.text, 'html.parser')
        
        
        for link in soup.find_all("a"):
          url=str(link.get('href'))
          if ".html" in url:
            vaildation=0
            for a in areas:
              if a in url:
                vaildation=1
            if  url not in self.articles and vaildation==1: 
              if "https://www.nytimes.com/" not in url and url[0]=='/':
                url="https://www.nytimes.com/"+url   
              self.articles.append(url)
              print(url)    
  def getarticle(self, x, d, m, year):
    print(" => downloading..... "+x)
    flage=1
    try:
      toi_article = Article(x, language="en") # en for English
      toi_article.download()
      toi_article.parse()
    except newspaper.article.ArticleException:
      flage=0
    
    if flage==1:
      #To extract title
      #print("Article's Title:")
      #print(toi_article.title)
      #print("n")
      #To extract text
      #print("Article's Text:")
      #print(toi_article.text)
      #print("n")
      #print(toi_article.publish_date)
      link=x
      title=str(toi_article.title)
      author=str(toi_article.authors)
      article=str(toi_article.text)
      source="The New York Times"
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(year)  
      month=months[m-1]
      DateOfPublication=year+"-"+month+"-"+day
      DateOfExtraction=str(datetime.date(datetime.now()))
      #print(source)
      #print(DateOfExtraction)
      #print(DateOfPublication)
      #print(title)
      #print(author)
      #print(article)
      return source, link, DateOfExtraction, DateOfPublication, title, author, article
    else:
      return "0","0","0","0","0","0" ,"0"  
  def extract(self, date1, date2, c):
    from datetime import datetime
    Datetoday=str(datetime.date(datetime.now()))
    print("\n => scraper: new york times news")



    if c==0: #defined range
      datei=date1.split('-',2)
      year=int(datei[0])
      m1=int(datei[1])
      d1=int(datei[2])
      datef=date2.split('-',2)
      year=int(datef[0])
      m2=int(datef[1])
      d2=int(datef[2])

      url=""
      chk=0
      for j in range(m1, m2+1):
          if(j==2):
              df=28
          if j in [4,5,7,8,10,12]:
              df=31       
          if j in [1,2,4,6,9,11]: 
              df=30
          di=1
          if j==m1:
              di=d1
          if j==m2:
              df=d2
          for i in range(di, df+1):
              self.articles.clear()
              self.downloaddata(i, j, year, url, chk)

"""### Scrapper: Dawn News"""

class Dawn(NewsExtractor):
  source="dawn"
  def getdatelinks(self, d, m, y):
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(y)  
      month=months[m-1]
      print(" => getting article links...........", day,month+",",year)
      import requests
      from bs4 import BeautifulSoup
      catagories=["1","2"]
        
      check="www.dawn.com/"
      #check1="/opinion/"
      for c in catagories:
        url="https://www.dawn.com/archive/"+year+"-"+month+"-"+str(day)
        print(">>>>>", url)
        reqs = requests.get(url)
        soup = BeautifulSoup(reqs.text, 'html.parser')
        
        for link in soup.find_all("a", {"class": "story__link"}):
        
            url=word=str(link.get('href'))
            if check in url:
                if  url not in self.articles:    
                  self.articles.append(url)
                  print(url)    
  def getarticle(self, x, d, m, year):
    print(" => downloading..... "+x)
    flage=1
    try:
      toi_article = Article(x, language="en") # en for English
      toi_article.download()
      toi_article.parse()
    except newspaper.article.ArticleException:
      flage=0
    
    if flage==1:
      #To extract title
      #print("Article's Title:")
      #print(toi_article.title)
      #print("n")
      #To extract text
      #print("Article's Text:")
      #print(toi_article.text)
      #print("n")
      #print(toi_article.publish_date)
      link=x
      title=str(toi_article.title)
      author=str(toi_article.authors)
      article=str(toi_article.text)
      source="Dawn"
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(year)  
      month=months[m-1]
      DateOfPublication=year+"-"+month+"-"+day
      DateOfExtraction=str(datetime.date(datetime.now()))
      #print(source)
      #print(DateOfExtraction)
      #print(DateOfPublication)
      #print(title)
      #print(author)
      #print(article)
      return source, link, DateOfExtraction, DateOfPublication, title, author, article
    else:
      return "0","0","0","0","0","0" ,"0"  
  def extract(self, date1, date2, c):
    from datetime import datetime
    Datetoday=str(datetime.date(datetime.now()))
    print("\n => scraper: Dawn news")



    if c==0: #defined range
      datei=date1.split('-',2)
      year=int(datei[0])
      m1=int(datei[1])
      d1=int(datei[2])
      datef=date2.split('-',2)
      year=int(datef[0])
      m2=int(datef[1])
      d2=int(datef[2])

      url=""
      chk=0
      for j in range(m1, m2+1):
          if(j==2):
              df=28
          if j in [4,5,7,8,10,12]:
              df=31       
          if j in [1,2,4,6,9,11]: 
              df=30
          di=1
          if j==m1:
              di=d1
          if j==m2:
              df=d2
          for i in range(di, df+1):
              self.articles.clear()
              self.downloaddata(i, j, year, url, chk)

"""### Scrapper: Tribune News"""

class tribune(NewsExtractor):
  source="tribune"
  def getdatelinks(self, d, m, y):
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(y)  
      month=months[m-1]
      print(" => getting article links...........", day,month+",",year)
      import requests
      from bs4 import BeautifulSoup
      catagories=["1"]

      #check="/article/"
      #check1="/opinion/"
      for c in catagories:
        
        url="https://tribune.com.pk/listing/web-archive/"+year+"-"+month+"-"+str(day)
        print(">>>>>", url)
        reqs = requests.get(url)
        soup = BeautifulSoup(reqs.text, 'html.parser')
        
        for link in soup.find_all("a"):
          
          url=str(link.get('href'))
          if "/story/" in url:
            if  url not in self.articles:    
              self.articles.append(url)
              #print(url)    
  def getarticle(self, x, d, m, year):
    print(" => downloading..... "+x)
    flage=1
    try:
      toi_article = Article(x, language="en") # en for English
      toi_article.download()
      toi_article.parse()
    except newspaper.article.ArticleException:
      flage=0
    
    if flage==1:
      #To extract title
      #print("Article's Title:")
      #print(toi_article.title)
      #print("n")
      #To extract text
      #print("Article's Text:")
      #print(toi_article.text)
      #print("n")
      #print(toi_article.publish_date)
      link=x
      title=str(toi_article.title)
      author=str(toi_article.authors)
      article=str(toi_article.text)
      source="Tribune"
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(year)  
      month=months[m-1]
      DateOfPublication=str(toi_article.publish_date)
      DateOfPublication=DateOfPublication.split(" ", 1)
      DateOfPublication=DateOfPublication[0]
      

      DateOfExtraction=str(datetime.date(datetime.now()))
      #print(source)
      #print(DateOfExtraction)
      #print(DateOfPublication)
      #print(title)
      #print(author)
      #print(article)
      return source, link, DateOfExtraction, DateOfPublication, title, author, article
    else:
      return "0","0","0","0","0","0" ,"0"  
  def extract(self, date1, date2, c):
    from datetime import datetime
    Datetoday=str(datetime.date(datetime.now()))
    print("\n => scraper: tribune news")



    if c==0: #defined range
      datei=date1.split('-',2)
      year=int(datei[0])
      m1=int(datei[1])
      d1=int(datei[2])
      datef=date2.split('-',2)
      year=int(datef[0])
      m2=int(datef[1])
      d2=int(datef[2])

      url=""
      chk=0
      for j in range(m1, m2+1):
          if(j==2):
              df=28
          if j in [4,5,7,8,10,12]:
              df=31       
          if j in [1,2,4,6,9,11]: 
              df=30
          di=1
          if j==m1:
              di=d1
          if j==m2:
              df=d2
          for i in range(di, df+1):
              self.articles.clear()
              self.downloaddata(i, j, year, url, chk)

"""### Scrapper Object Initilization & Directory Path Set"""

o1=TOI()
o1.setdirpath("F:/xampp1/htdocs/extraction/datasets/toi")
o2=NYT()
o2.setdirpath("F:/xampp1/htdocs/extraction/datasets/NYT")
o3=Dawn()
o3.setdirpath("F:/xampp1/htdocs/extraction/datasets/dawn")
o4=tribune()
o4.setdirpath("F:/xampp1/htdocs/extraction/datasets/tribune")

"""### Scrapper Exectuation Daily"""

#o1.daily()
#o2.daily()
#o3.daily()
#o4.daily()

"""### Scrapper Exectuation In Date Range """

#date1: FROM date to start news etraction
#date2: TO date to end news etraction
date1="2021-12-02"    
date2="2021-12-22"

#Call parser objects in date-range
#o1.rangedate(date1, date2)
#o2.rangedate(date1, date2)
#o3.rangedate(date1, date2)
date1="2021-12-09"    
date2="2021-12-22"
o4.rangedate(date1, date2)

"""### Scrapper Exectuation Weekly"""

#o1.weekly()
#o2.weekly()
#o3.weekly()
#o4.weekly()

"""### Scrapper Exectuation Monthly"""

#o1.monthly()
#o2.monthly()
#o3.monthly()
#o4.monthly()