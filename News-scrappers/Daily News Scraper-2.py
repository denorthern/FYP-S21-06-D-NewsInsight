# -*- coding: utf-8 -*-
"""Final_Newextraction_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eEdaDvfifptNlMBQmJy3KqD7nO1Txmr_

# **News Scrapper**
News Sources: Sputnik - Tass - SCMP - Xinhua
"""

import os
import sys
import time
import requests
import threading
import collections
import numpy as np

from datetime import datetime
from datetime import datetime
from bs4 import BeautifulSoup

from urllib.error import HTTPError
"""### Scrapper parent class: NewsExtractor
"""

class NewsExtractor:
  articles=[]
  dir=""
  date=""
  def setdirpath(self, path):
      # Check path exists
      Existflag = os.path.exists(path)
      if not Existflag:
      # Creating new directory as it not exists 
        os.makedirs(path)
      self.dir=path
      path=path+"-byDate"
      # Check path exists
      Existflag = os.path.exists(path)
      if not Existflag:
      # Creating new directory as it not exists 
        os.makedirs(path)
    
   
  def downloadarticles(self, i, array1):  
  """## Member function: downloadarticles
  #input( thread no, splited array of urls for thread)
  """
      le=len(array1)
      print(" threade:  => ", i)
      c=0
      for q in array1:                      
          print(" thread: ", i, "  => (",  c,"/",le,")")
          x=q
          error1=0 #error#1 check set to fasle  
          # Get ('source, 'link', 'date extraction', 
          # 'date publication', 'author', 'article')
          # From function getarticle( Article url )
          try:
            (s, l, De, Dp, t, aut, art)=self.getarticle(x)
          except TypeError: 
            error1=1 
            # Expection for getarticle(url)
            #error#1 check set to true
          #-----------------------------------------  
          if error1!=1:  #for false error#1
              if (s+l+De+Dp+t+aut+art!="0"+"0"+"0"+"0"+"0"+"0"+"0"):
              # if required var contain required data 
                  non=['/',':','*','?','"','<','>','|','\n', '\\', '\r-'] 
                  #file naming illegal chars removing
                  name=s+" _ "+Dp+" _ "+t+".txt"
                  for n in non:
                      name=name.replace(n, '-')
                  #---------------------------------
                  #create file against source & write data
                  f = open(self.dir+"/"+name, "w", encoding="utf-8")
                  f.write("Source: " + s)
                  f.write("\nLink: " + l)
                  f.write("\nDateOfExtraction: " + De)
                  f.write("\nDateOfPublication: " + Dp)
                  f.write("\nTitle: " + t)
                  f.write("\nAuthor: " + aut)
                  f.write("\nArticle: " + art)
                  f.close()
                  #---------------------------------
                  #file naming illegal chars removing
                  name=t+".txt"
                  for n in non:
                      name=name.replace(n, '-')
                  #---------------------------------   
                  #create file against date & write data
                  path=self.dir+"-byDate"+"/"+Dp
                  # Check path exists
                  Existflag = os.path.exists(path)
                  if not Existflag:
                  # Creating new directory as it not exists 
                    os.makedirs(path)        
                  f = open(path+"/"+name, "w", encoding="utf-8")
                  f.write("Source: " + s)
                  f.write("\nLink: " + l)
                  f.write("\nDateOfExtraction: " + De)
                  f.write("\nDateOfPublication: " + Dp)
                  f.write("\nTitle: " + t)
                  f.write("\nAuthor: " + aut)
                  f.write("\nArticle: " + art)
                  f.close()
                  #---------------------------------
                  #create record of newly added urls
                  NewlyAddedFiles = "F:/xampp1/htdocs/extraction/urlrecords/NewlyAddedFilesUrl/"+Dp+"/"+self.source+".txt"
                  file2 = open(NewlyAddedFiles,"a+")#append mode
                  file2.write(path+"/"+name+" \n") 
                  file2.close()
                  #---------------------------------
          c=c+1 #counter
      print(" => download complete! thread: ", i) 
  
  def downloaddata(self, d, m, year, url, chk ):
  """## Member function: downloadata
  #input( Date:day, month, year, Url to scrape, check flag)
  """    
      if chk==0: #date based getting articles
        self.getdatelinks(d, m, year)
      else:
        self.getdatelinks1(url) #url based getting articles
      #list articles
      self.articles = list(dict.fromkeys(self.articles))
      #Date formatting
      day=int(d)
      month=int(m)
      m2=str(month)
      d2=str(day)
      #month formatting
      if month<10:
        m2="0"+str(month)
      d2=day
      #day formatting
      if day<10:
        d2="0"+str(day)
      
      #getting previous date
      day=day-1
      if day==0:
          if(month==3):
              day=28
          if month in [4,5,7,8,10,12]:
              day=30
          if month in [1,2,4,6,9,11]: 
              day=31
          month=month-1
          if(month==0):
              month=12
      m1=month
      if month<10:
        m1="0"+str(month)
      d1=day
      if day<10:
        d1="0"+str(day)
      # -------------------------------
      #setting current ectraction date
      self.date=str(year)+"-"+str(m2)+"-"+str(d2)
      #getting old urls records
      old1=[] # old url records
      path = "F:/xampp1/htdocs/extraction/urlrecords/"+str(year)+"-"+str(m1)+"-"+str(d1)
      filename=path+"/"+self.source+".txt"
      # Check path exists
      Existflag = os.path.exists(path)
      if not Existflag: #if not exsist
        pass
      else: #if exsist
        file1 = open(filename,"r+")
        old1=file1.readlines()     
        file1.close()
      # ------------------------------ 
      
      path = "F:/xampp1/htdocs/extraction/urlrecords/"+str(year)+"-"+str(m2)+"-"+str(d2)
      # Check file path exists
      Existflag = os.path.exists(path)
      if not Existflag:
      # Creating new directory as it not exists 
        os.makedirs(path)
      filename=path+"/"+self.source+".txt"
      file1 = open(filename ,"a+")#append mode
      file1.close()
      
      file1 = open(filename,"r+")
      old=file1.readlines()      
      temp=[]
      file1 = open(filename,"a+")#append mode
      for item in self.articles:
        if (item+" \n") in old or (item+" \n") in old1:
          self.articles.remove(item)
        else:
          temp.append(item)
          file1.write(item+" \n") 
          
      file1.close()
      self.articles=temp
      #--------------------Creating Newly Added url file------------#
      path = "F:/xampp1/htdocs/extraction/urlrecords/NewlyAddedFilesUrl/"+str(year)+"-"+str(m2)+"-"+str(d2)
      Existflag = os.path.exists(path)
      if not Existflag:
        os.makedirs(path)
      filename=path+"/"+self.source+".txt"
      file1 = open(filename,"w")
      file1.close()
      # ------------------------------------------------------------#

      #print(len( articles))
      print(" => links extracted :" + str(len(self.articles)))
      arr = np.array(self.articles)
      splitno=1
      if len( self.articles) > 1000:
          splitno=14 #change according to number of articles must be less then no of articles 
      if len( self.articles) > 500 and len( self.articles) <= 1000:
          splitno=11 #change according to number of articles must be less then no of articles 
      if len( self.articles) > 100 and len( self.articles) <= 500:
          splitno=8 #change according to number of articles must be less then no of articles  
      if len( self.articles) >20 and len( self.articles) <= 100:
          splitno=4
      if len( self.articles) <20:
          splitno=1  
      newarr = np.array_split(arr, splitno) #split array into sub "splitno" of arrays
      i=0
      #Multi threading passing each split of urls to downloadarticles against thread
      while(i<splitno):
          a=newarr[i]
          t1 = threading.Thread(target=self.downloadarticles, args=(i,a,))
          t1.start()
          time.sleep(8)
          i=i+1
      t1.join()  
      #witing all threads to complete

  def daily(self):
  """## Member function: daily
  #input()
  #set date range for daily article extraction
  """   
      print(" => Daily news article parser")
      Datetoday=str(datetime.date(datetime.now())) #current date-time
      #date formatting and split
      today=Datetoday.split('-',2)
      year=int(today[0])
      month=int(today[1])
      m2=month
      #month check
      if month<10:
        m2="0"+str(month)
      day=int(today[2])
      d2=day
      #day check
      if day<10:
        d2="0"+str(day)
      #getting day before date  
      day=day-1
      if day==0:
          if(month==3):
              day=28
          if month in [4,5,7,8,10,12]:
              day=30
          if month in [1,2,4,6,9,11]: 
              day=31
          month=month-1
          if(month==0):
              month=12
      m1=month
      #month check
      if month<10:
        m1="0"+str(month)
      d1=day
      #day check
      if day<10:
        d1="0"+str(day)
      date1="2021-01-01"
      date2="2021-01-01"
      date1=str(year)+"-"+str(m1)+"-"+str(d1)
      date2=str(year)+"-"+str(m2)+"-"+str(d2)
      #----------------------------  
      #passing date1 & date2 to article extraction by date range
      self.rangedate(date1, date2)

  def weekly(self):
      print(" => weekly news article parser")

      Datetoday=str(datetime.date(datetime.now()))
      today=Datetoday.split('-',2)
      year=int(today[0])
      month=int(today[1])
      day=int(today[2])
      day=day-1
      if day==0:
          if(month==3):
              day=28
          if month in [4,5,7,8,10,12]:
              day=30
          if month in [1,2,4,6,9,11]: 
              day=31
          month=month-1
          if(month==0):
              month=12
      print(" ----", Datetoday, "-----\n")
      #input1 from: 
      m2=month #month
      d2=day #day
      m1=month #month
      d1=day #day

      c=2
      if c==2: 
        d1=d2-7
        if d1<=0:
          if(month==3):
            day=28
          if month in [3,5,7,8,10,12]:
            day=30    
          if month in [1,2,4,6,9,11]: 
            day=31 
          d1=day+d1
          m1=month-1
          if(m1==0):
            month=12
        else:
          m1=m2  
      date1=str(year)+"-"+str(m1)+"-"+str(d1)
      date2=str(year)+"-"+str(m2)+"-"+str(d2)
      self.rangedate(date1, date2)

  def monthly(self):
      print(" => monthly news article parser")
      Datetoday=str(datetime.date(datetime.now()))
      today=Datetoday.split('-',2)
      year=int(today[0])
      month=int(today[1])
      day=int(today[2])
      day=day-1
      if day==0:
          if(month==3):
              day=28
          if month in [4,5,7,8,10,12]:
              day=30       
          if month in [1,2,4,6,9,11]: 
              day=31
          month=month-1
          if(month==0):
              month=12
      #input1 from: 
      m2=month #month
      d2=day #day
      m1=month #month
      d1=day #day

      c=3
      if c==3:  #monthly
        m1=m2-1
        if(m1==0):
          m1=12
      date1=str(year)+"-"+str(m1)+"-"+str(d1)
      date2=str(year)+"-"+str(m2)+"-"+str(d2)
      self.rangedate(date1, date2)

  def getarticle(self, x):
  """## Member function: getarticle
  #input(article url)
  #extract article data against url
  """ 
    pass  

  def getdatelinks(self, d, m, year):
  """## Member function: getdatelinks
  #input(date)
  #extract article urls against date
  """ 
    pass 

  def extract(self, d, m, year):
  """## Member function: extract
  #input(Date)
  #set extraction creteria for article extrction
  """ 
    pass 

  def rangedate(self, date1, date2):
  """## Member function: rangedate
  #input(date)
  #pass dates to extract
  """ 
      print(" => specified news article parser")
      c=0
      self.extract(date1, date2, c)

"""### Scrapper: Sputnik News"""

class Sputnik(NewsExtractor):
  source="sputnik"

  def getdatelinks(self, d, m, y):
  """## Member function: getdatelinks
  #input(date)
  #get urls
  """ 
      #date reformating
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(y)  
      month=months[m-1]
      print(" => getting article links...........", day,month+",",year)
      date1=year+month+day
      #----------------
      #source categories & split criterias & date check
      catagories=["","search/?query=","?"]
      catagories[0]=date1
      split="https://sputniknews.com"
      #/20210723
      datechk="/"+year+month+str(day)
      #----------------------------------------------
      #iterate categories: extract urls
      for c in catagories:
        #beauitful soup
        url="https://sputniknews.com/"+c
        reqs = requests.get(url)
        soup = BeautifulSoup(reqs.text, 'html.parser')
        #--------------
        for link in soup.find_all("a"):
              url=str(link.get('href'))
              if (".html" in url):
                if split not in url:
                  url=split+url
                if datechk in url and url not in self.articles:  
                    
                    self.articles.append(url)  
                    print(url)    
      #------------------------------
  def getarticle(self, x):
    """
    # Member function: getarticle
    # input(url) return Article data
    """
    #beauitful soup
    print(" => downloading..... "+x)
    reqs = requests.get(x)
    soup = BeautifulSoup(reqs.text, 'lxml')
    #-------------------
    #soup.find_element_by_class_name('article__title')   
    #txts=soup.find_elements_by_class_name('article__text')
    #getting required data
    txts = soup.findAll("div", {"class": "article__text"})
    text=""
    for txt in txts:
      text=text+txt.text
    source="sputniknews"
    author="sputniknews"
    #title = soup.find_element_by_class_name('article__title')
    title = soup.find("h1", {"class": "article__title"}).text
    DateOfPublication=self.date
    link=x
    DateOfExtraction=str(datetime.date(datetime.now()))
    article=text
    #----------------------
    # Result: (source, link, DateOfExtraction, DateOfPublication, title, author, article
    return source, link, DateOfExtraction, DateOfPublication, str(title), author, str(article)
    #-------------------------------------------------------------------------------------
   
  def extract(self, date1, date2, c):

    Datetoday=str(datetime.date(datetime.now()))
    print("\n => scraper: sputnik news", " - date:", Datetoday)
  
    if c==0: #defined range
      print(" to:",date1, "----- from:",date2,"\n")
      datei=date1.split('-',2)
      year=int(datei[0])
      m1=int(datei[1])
      d1=int(datei[2])
      datef=date2.split('-',2)
      year=int(datef[0])
      m2=int(datef[1])
      d2=int(datef[2])

      url=""
      chk=0
      for j in range(m1, m2+1):
          if(j==3):
              df=28
          if j in [4,5,7,8,10,12]:
              df=31       
          if j in [1,2,4,6,9,11]: 
              df=30
          di=1
          if j==m1:
              di=d1
          if j==m2:
              df=d2
          for i in range(di, df+1):
              self.articles.clear()
              self.downloaddata(i, j, year, url, chk)
    else:
      if c==1:  #daily
        url="/html/body/div[2]/div[8]/div[1]/div[1]/div[2]/form/div[1]/div/ul/li[1]"
      if c==2: 
        url="/html/body/div[2]/div[8]/div[1]/div[1]/div[2]/form/div[1]/div/ul/li[2]"
      if c==3:  #monthly
        url="/html/body/div[2]/div[8]/div[1]/div[1]/div[2]/form/div[1]/div/ul/li[3]"
      
      i=0
      j=0
      year=0
      chk=1
      self.articles.clear()  
      self.downloaddata(i, j, year, url, chk)

"""### Scrapper: SCMP News"""

class Scmp(NewsExtractor):
  source="scmp"

  def getdatelinks(self, d, m, y):
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(y)  
      month=months[m-1]
      print(" => getting article links...........", day,month+",",year)
      date1=year+"-"+month+"-"+day

      catagories=["","asia", "world", "sport","hong-kong","business","economy","sport","tech"]

      check="/article/"
      check1="/opinion/"
      split="https://www.scmp.com"
      check3=".scmp.com/"
      for c in catagories:
        #url="https://web.archive.org/web/"+year+month+str(day)+"032150"+"/https://www.scmp.com/news/"+c
        url="https://www.scmp.com/news/"+c
        reqs = requests.get(url)
        soup = BeautifulSoup(reqs.text, 'html.parser')
        for link in soup.find_all('a'):
          if check in str(link.get('href')) and check1 not in str(link.get('href')):
            word=link.get('href')
            url=word
            if split not in url and check3 not in url:
              url=split+url
               
            print(url)  
            self.articles.append(url)
  
  def getarticle(self, x):
    
    errflage=0
    print(" => downloading..... "+x)
    try:
        reqs = requests.get(x)
        reqs.raise_for_status()
    except HTTPError as hp:    
        errflage=1
   # except URLError as ue:   
    #    errflage=1
    if(errflage!=1):      
        soup = BeautifulSoup(reqs.text, 'lxml')
        #soup.find_element_by_class_name('article__title')   
        #txts=soup.find_elements_by_class_name('article__text')
        text=""
        for i in [0,1]:
          if i==0:
            try:
                txts = soup.findAll("p", {"class": "content--p"})
            except AttributeError:
                errflage=1
          if i==1:
            try:
                txts = soup.findAll("li", {"class": "content--li"})
            except AttributeError:
                errflage=1
          if(errflage!=1):  
              for txt in txts:
                text=text+txt.text
        source="scmp"
        author="scmp"
        #title = soup.find_element_by_class_name('article__title')
        try:
            title = soup.find("h1", {"class": "info__headline"}).text
        except AttributeError:
            errflage=1  
        if(errflage!=1):    
            DateOfPublication=self.date
            link=x
            DateOfExtraction=str(datetime.date(datetime.now()))
            article=text
            return source, link, DateOfExtraction, DateOfPublication, str(title), author, str(article) 
    else: 
        return "0", "0", "0", "0", "0", "0", "0"
 
  def extract(self, date1, date2, c):
    Datetoday=str(datetime.date(datetime.now()))
    today=Datetoday.split('-',2)
    year=int(today[0])
    month=int(today[1])
    day=int(today[2])
    day=day-1
    if day==0:
        if(month==3):
            day=28
        if month in [4,5,7,8,10,12]:
            day=30
            
        if month in [1,2,4,6,9,11]: 
            day=31
        month=month-1
        if(month==0):
            month=12

    print("\n => scraper: scmp news", " - date:", Datetoday)
    #input1 from: 
    m2=month #month
    d2=day #day
    m1=month #month
    d1=day #day
    #input2 to:
    
    if c==0: #defined range
      print(" to:",date1, "----- from:",date2,"\n")
      datei=date1.split('-',2)
      year=int(datei[0])
      m1=int(datei[1])
      d1=int(datei[2])
      datef=date2.split('-',2)
      year=int(datef[0])
      m2=int(datef[1])
      d2=int(datef[2])

    if c==1:  #daily
      m1=m2 #month
      d1=d2 #day
    
    url=""
    chk=0
    for j in range(m1, m2+1):
          if(j==3):
              df=28
          if j in [4,5,7,8,10,12]:
              df=31       
          if j in [1,2,4,6,9,11]: 
              df=30
          di=1
          if j==m1:
              di=d1
          if j==m2:
              df=d2
          for i in range(di, df+1):
              self.articles.clear()
              self.downloaddata(i, j, year, url, chk)

"""### Scrapper: Xinhua News"""

class Xinhua(NewsExtractor):
  source="xinhua"

  def getdatelinks(self, d, m, y):
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(y)  
      month=months[m-1]
      print(" => getting article links...........", day,month+",",year)
      date1=year+month+day
      #print("chkdate", date)
      catagories=["","china/","list/china-business.htm","culture/","world/","sports/","asiapacific/2018/","europe/2018/","africa/2018/",
                  "list/latestnews.htm"]
      catagories[0]=date1
      split="http://www.xinhuanet.com/english/"
      #/2021-11/16/
      datechk="/"+year+"-"+month+"/"+str(day)+"/"
      for c in catagories:
        url="http://www.xinhuanet.com/english/"+c
        reqs = requests.get(url)
        soup = BeautifulSoup(reqs.text, 'html.parser')
        
        for link in soup.find_all("a"):
              #print(link.get('href'))
              url=str(link.get('href'))
              if (".htm" in url):
                
                if datechk in url and url not in self.articles:  
                    
                    self.articles.append(url)  
                    print(url)    

  def getarticle(self, x):
    print(" => downloading..... "+x)
    
    reqs = requests.get(x)
    soup = BeautifulSoup(reqs.text, 'lxml')
    #soup.find_element_by_class_name('article__title')   
    #txts=soup.find_elements_by_class_name('article__text')
    txts = soup.findAll("p")
    text=""
    for txt in txts:
      text=text+txt.text
    source="xinhua"
    author="xinhua"
    #title = soup.find_element_by_class_name('article__title')
    title = soup.find("h1", {"class": "Btitle"}).text
    DateOfPublication=self.date
    link=x
    DateOfExtraction=str(datetime.date(datetime.now()))
    article=text
    return source, link, DateOfExtraction, DateOfPublication, str(title), author, str(article) 

  def extract(self, date1, date2, c):
    Datetoday=str(datetime.date(datetime.now()))
    today=Datetoday.split('-',2)
    year=int(today[0])
    month=int(today[1])
    day=int(today[2])
    day=day-1
    if day==0:
        if(month==3):
            day=28
        if month in [4,5,7,8,10,12]:
            day=30
            
        if month in [1,2,4,6,9,11]: 
            day=31
        month=month-1
        if(month==0):
            month=12
    print("\n => scraper: xinhua news", " - date:", Datetoday)
    #input1 from: 
    m2=month #month
    d2=day #day
    m1=month
    d1=day
    #input2 to:

    if c==0: #defined range
      print(" to:",date1, "----- from:",date2,"\n")
      datei=date1.split('-',2)
      year=int(datei[0])
      m1=int(datei[1])
      d1=int(datei[2])
      datef=date2.split('-',2)
      year=int(datef[0])
      m2=int(datef[1])
      d2=int(datef[2])

    if c==1:  #daily
      m1=m2 #month
      d1=d2 #day
    
    url=""
    chk=0
    for j in range(m1, m2+1):
          if(j==3):
              df=28
          if j in [4,5,7,8,10,12]:
              df=31       
          if j in [1,2,4,6,9,11]: 
              df=30
          di=1
          if j==m1:
              di=d1
          if j==m2:
              df=d2
          for i in range(di, df+1):
              self.articles.clear()
              self.downloaddata(i, j, year, url, chk)

"""### Scrapper: Tass News"""

class Tass(NewsExtractor):
  source="tass"

  def getdatelinks(self, d, m, y):
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(y)  
      month=months[m-1]
      print(" => getting article links...........", day,month+",",year)
      catagories=["", "politics", "world", "sport", "economy", "defense" ]

      #check="/article/"
      #check1="/opinion/"
      split="https://tass.com/"
      for c in catagories:
        url="https://web.archive.org/web/"+year+month+str(day)+"032150"+"/https://tass.com/"+c
        reqs = requests.get(url)
        soup = BeautifulSoup(reqs.text, 'html.parser')
        
        for link in soup.find_all("a", {"class": "news-preview"}):
          #print(link.get('class'))
          if  str(link.get('href')) not in self.articles:
            word=str(link.get('href'))
            #print(word)
            
            wordlist=word.split(split, 1)
            if len(word)==len(wordlist):
              w=wordlist[0]
              #print(w)
            else:  
              if len(wordlist)>1:
                #print(wordlist)
                url=split+wordlist[1]
                #print(url)
              else:
                url=wordlist 
            
            if url not in self.articles:     
              self.articles.append(url)
              print(url)
  def getarticle(self, x):
    print(" => downloading..... "+x)
    reqs = requests.get(x)
    soup = BeautifulSoup(reqs.text, 'lxml')
    #soup.find_element_by_class_name('article__title')   
    #txts=soup.find_elements_by_class_name('article__text')
    txts = soup.findAll("p")
    text=""
    for txt in txts:
      text=text+txt.text
    source="tass"
    author="tass"
    #title = soup.find_element_by_class_name('article__title')
    title = soup.find("h1", {"class": "news-header__title"}).text
    DateOfPublication=self.date
    link=x
    DateOfExtraction=str(datetime.date(datetime.now()))
    article=text
    return source, link, DateOfExtraction, DateOfPublication, title, author, article             

  def extract(self, date1, date2, c):
    Datetoday=str(datetime.date(datetime.now()))
    print("\n => scraper: tass news", " - date:", Datetoday)
    if c==0: #defined range
      print(" to:",date1, "----- from:",date2,"\n")
      datei=date1.split('-',2)
      year=int(datei[0])
      m1=int(datei[1])
      d1=int(datei[2])
      datef=date2.split('-',2)
      year=int(datef[0])
      m2=int(datef[1])
      d2=int(datef[2])
      
      url=""
      chk=0
      for j in range(m1, m2+1):
          if(j==3):
              df=28
          if j in [4,5,7,8,10,12]:
              df=31       
          if j in [1,2,4,6,9,11]: 
              df=30
          di=1
          if j==m1:
              di=d1
          if j==m2:
              df=d2
          for i in range(di, df+1):
              self.articles.clear()
              self.downloaddata(i, j, year, url, chk)

    else:
      if c==1:  #daily
        url="https://tass.com/search?sort=date&range=day"
      if c==2: 
        url="https://tass.com/search?sort=date&range=week"
      if c==3:  #monthly
        url="https://tass.com/search?sort=date&range=month"
      
      i=0
      j=0
      year=0
      chk=1
      self.articles.clear()  
      self.downloaddata(i, j, year, url, chk)

"""### Scrapper Object Initilization & Directory Path Set"""

o1=Tass()
o1.setdirpath("F:/xampp1/htdocs/extraction/datasets/tass")
o2=Sputnik()
o2.setdirpath("F:/xampp1/htdocs/extraction/datasets/sputnik")
o3=Xinhua()
o3.setdirpath("F:/xampp1/htdocs/extraction/datasets/xinhua")
o4=Scmp()
o4.setdirpath("F:/xampp1/htdocs/extraction/datasets/scmp")

"""### Scrapper Exectuation Daily"""

o1.daily()
o2.daily()
o3.daily()
o4.daily()

"""### Scrapper Exectuation In Date Range """

#date1: FROM date to start news etraction
#date2: TO date to end news etraction
#date1="2021-07-01"    
#date2="2021-07-01"

#Call parser objects in date-range
#o1.rangedate(date1, date2)
#o2.rangedate(date1, date2)
#o3.rangedate(date1, date2)
#o4.rangedate(date1, date2)

"""### Scrapper Exectuation Weekly"""

#o1.weekly()
#o2.weekly()
#o3.weekly()
#o4.weekly()

"""### Scrapper Exectuation Monthly"""

#o1.monthly()
#o2.monthly()
#o3.monthly()
#o4.monthly()