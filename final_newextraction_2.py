# -*- coding: utf-8 -*-
"""Final_Newextraction_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-jFtjBHVGl86KXyrGATb7E8oT1Tvc_Ee

# **News Scrapper**
News Sources: Sputnik - Tass - SCMP - Xinhua
"""

!pip install newspaper3k

from newspaper import Article
import newspaper
import time
import collections
from datetime import datetime
from datetime import date 
from datetime import timedelta
import collections
def daterange(start, end):
      for n in range(int ((end-start).days)):
        yield start+timedelta(n)

"""### Google Drive Mount"""

from google.colab import drive
drive.mount('/content/drive')

"""### Scrapper parent class: NewsExtractor"""

class NewsExtractor:
  articles=collections.deque()
  dir=""

  def setdirpath(self, path):
      self.dir=path
  def downloadarticles(self, i, array1,d, m, year):
      #print(" => downloading articles. .  .  .    . thread: ", i)
      #from tqdm import tqdm
      le=len(array1)
      #pbar = tqdm(total=len(array))
      print(" threade:  => ", i)
      c=0
      #d=webdriver.Chrome('chromedriver',chrome_options=chrome_options) #Path to Chrome Driver
      for q in array1:
          print(" threade:", i, "  => (",  c,"/",le,")")
          x=q
          (s, l, De, Dp, t, aut, art)=self.getarticle(x, d, m, year)
          if (s+l+De+Dp+t+aut+art!="0"+"0"+"0"+"0"+"0"+"0"+"0"):
              non=['/',':','*','?','"','<','>','|']
              name=s+" _ "+Dp+" _ "+t+".txt"
              for n in non:
                  name=name.replace(n, '-')
              #from pathlib import Path
              #Path(self.dir+Dp+"/").mkdir(parents=True, exist_ok=True)
              #f = open(self.dir+Dp+"/"+name, "w", encoding="utf-8")
              f = open(self.dir+name, "w", encoding="utf-8")
              f.write("Source: " + s)
              f.write("\nLink: " + l)
              f.write("\nDateOfExtraction: " + De)
              f.write("\nDateOfPublication: " + Dp)
              f.write("\nTitle: " + t)
              f.write("\nAuthor: " + aut)
              f.write("\nArticle: " + art)
              f.close()
              #pbar.update(1)
              #print(" threadw:  => ", i)
          c=c+1
      #pbar.close()
      print(" => download complete! thread: ", i)
  
  def downloaddata(self, d, m, year, url, chk ):
      
      import numpy as np

      if chk==0:
        self.getdatelinks(d, m, year)
      else:
        self.getdatelinks(url)

      #print(len( articles))
      print(" => links extracted :" + str(len(self.articles)))
      arr = np.array(self.articles)
      splitno=1
      if len( self.articles) > 1000:
          splitno=14 #change according to number of articles must be less then no of articles 
      if len( self.articles) > 500 and len( self.articles) <= 1000:
          splitno=11 #change according to number of articles must be less then no of articles 
      if len( self.articles) > 100 and len( self.articles) <= 500:
          splitno=8 #change according to number of articles must be less then no of articles  
      if len( self.articles) >20 and len( self.articles) <= 100:
          splitno=5
      if len( self.articles) <20:
          splitno=1  
      newarr = np.array_split(arr, splitno)
      import threading
      i=0
      while(i<splitno):
          a=newarr[i]
          t1 = threading.Thread(target=self.downloadarticles, args=(i,a,d, m, year,  ))
          t1.start()
          time.sleep(8)
          i=i+1
      t1.join()  
  def daily(self):
      print(" => Daily news article parser")

      Datetoday=str(datetime.date(datetime.now()))
      today=Datetoday.split('-',2)
      year=int(today[0])
      month=int(today[1])
      day=int(today[2])
      day=day-1
      if day==0:
          if(month==3):
              day=28
          if month in [4,5,7,8,10,12]:
              day=30
              
          if month in [1,2,4,6,9,11]: 
              day=31
          month=month-1
          if(month==0):
              month=12

      #input1 from: 
      m2=month #month
      d2=day #day
      m1=month #month
      d1=day #day

      date1=str(year)+"-"+str(m1)+"-"+str(d1)
      date2=str(year)+"-"+str(m2)+"-"+str(d2)

      self.rangedate(date1, date2)
      
  def weekly(self):
      print(" => weekly news article parser")

      Datetoday=str(datetime.date(datetime.now()))
      today=Datetoday.split('-',2)
      year=int(today[0])
      month=int(today[1])
      day=int(today[2])
      day=day-1
      if day==0:
          if(month==3):
              day=28
          if month in [4,5,7,8,10,12]:
              day=30
              
          if month in [1,2,4,6,9,11]: 
              day=31
          month=month-1
          if(month==0):
              month=12

      #input1 from: 
      m2=month #month
      d2=day #day
      m1=month #month
      d1=day #day

      c=2
      if c==2: 
        d1=d2-7
        if d1<=0:
          if(month==3):
            day=28
          if month in [3,5,7,8,10,12]:
            day=30    
          if month in [1,2,4,6,9,11]: 
            day=31 
          d1=day+d1
          m1=month-1
          if(m1==0):
            month=12
        else:
          m1=m2  

      date1=str(year)+"-"+str(m1)+"-"+str(d1)
      date2=str(year)+"-"+str(m2)+"-"+str(d2)

      self.rangedate(date1, date2)

  def monthly(self):
      print(" => monthly news article parser")

      Datetoday=str(datetime.date(datetime.now()))
      today=Datetoday.split('-',2)
      year=int(today[0])
      month=int(today[1])
      day=int(today[2])
      day=day-1
      if day==0:
          if(month==3):
              day=28
          if month in [4,5,7,8,10,12]:
              day=30
              
          if month in [1,2,4,6,9,11]: 
              day=31
          month=month-1
          if(month==0):
              month=12
      #input1 from: 
      m2=month #month
      d2=day #day
      m1=month #month
      d1=day #day

      c=3
      if c==3:  #monthly
        m1=m2-1
        if(m1==0):
          m1=12
      
      date1=str(year)+"-"+str(m1)+"-"+str(d1)
      date2=str(year)+"-"+str(m2)+"-"+str(d2)
      self.rangedate(date1, date2)

  def getarticle(self, x):
    pass  
  def getdatelinks(self, d, m, year):
    pass 
  def extract(self, d, m, year):
    pass    
  def rangedate(self, date1, date2):
      print(" => specified news article parser")
      c=0
      self.extract(date1, date2, c)

"""### Scrapper: Times Of India News"""

class TOI(NewsExtractor):

  def getdatelinks(self, d, m, y):
      months = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'] 
      
      day=str(d)
      year=str(y)  
      month=months[m-1]
      print(" => getting article links...........", day,month+",",year)
      import requests
      from bs4 import BeautifulSoup
      catagories=["1"]

      #check="/article/"
      #check1="/opinion/"
      starttime=44269
      startmonth=3
      startday=14
      
      start=date(2021, startmonth, startday)
      end=date(y, m, d)
      difference=0
      for singledate in daterange(start, end):
        #print(singledate.strftime("%Y-%m-%d"))
        difference=difference=+1
      newtime=starttime+difference

      areas=["/world/", "/viral-news/", "/most-searched-products/", "/politics/",  "/sports/", "/entertainment/hindi/", "/entertainment/english/", "/life-style/", "/education/", "/elections/", "/india/", "/business/"]

      for c in catagories:
        #https://timesofindia.indiatimes.com/2021/3/14/archivelist/year-2021,month-3,starttime-44269.cms
        url="https://timesofindia.indiatimes.com/"+year+"/"+month+"/"+str(day)+"/archivelist/year-"+year+",month-"+month+",starttime-"+str(newtime)+".cms"
        print(">>>>>", url)
        reqs = requests.get(url)
        soup = BeautifulSoup(reqs.text, 'html.parser')
        
        for link in soup.find_all("a"):
            if "/articleshow/" in str(link.get('href')):
              url=str(link.get('href'))
              validation=0
              for area in areas:
                if area in url:
                  validation=1
              if  url not in self.articles and validation==1:    
                self.articles.append(url)
                #print(url)    
  def getarticle(self, x, d, m, year):
    print(" => downloading..... "+x)
    flage=1
    try:
      toi_article = Article(x, language="en") # en for English
      toi_article.download()
      toi_article.parse()
    except newspaper.article.ArticleException:
      flage=0
    
    if flage==1:
      #To extract title
      #print("Article's Title:")
      #print(toi_article.title)
      #print("n")
      #To extract text
      #print("Article's Text:")
      #print(toi_article.text)
      #print("n")
      #print(toi_article.publish_date)
      link=x
      title=str(toi_article.title)
      author=str(toi_article.authors)
      article=str(toi_article.text)
      source="Times of india"
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(year)  
      month=months[m-1]
      DateOfPublication=year+"-"+month+"-"+day
      DateOfExtraction=str(datetime.date(datetime.now()))
      #print(source)
      #print(DateOfExtraction)
      #print(DateOfPublication)
      #print(title)
      #print(author)
      #print(article)
      return source, link, DateOfExtraction, DateOfPublication, title, author, article
    else:
      return "0","0","0","0","0","0" ,"0"  
  def extract(self, date1, date2, c):
    from datetime import datetime
    Datetoday=str(datetime.date(datetime.now()))
    print("\n => scraper: times of india news")



    if c==0: #defined range
      print(" to:",date1, "----- from:",date2,"\n")
      datei=date1.split('-',2)
      year=int(datei[0])
      m1=int(datei[1])
      d1=int(datei[2])
      datef=date2.split('-',2)
      year=int(datef[0])
      m2=int(datef[1])
      d2=int(datef[2])

      url=""
      chk=0
      for j in range(m1, m2+1):
          df=31
          di=1
          if j==m1:
              di=d1
          if j==m2:
              df=d2
          for i in range(di, df+1):
              self.articles.clear()
              self.downloaddata(i, j, year, url, chk)

"""### Scrapper: The New-York Times News"""

class NYT(NewsExtractor):

  def getdatelinks(self, d, m, y):
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(y)  
      month=months[m-1]
      print(" => getting article links...........", day,month+",",year)
      import requests
      from bs4 import BeautifulSoup
      catagories=["World","U.S.", "Sports", "Style", "Business", "Movies", "Opinion", "Books", "Arts"]
      areas=["/world/","/us/", "/sports/", "/style/", "/business/", "/movies/", "/opinion/", "/books/", "/arts/"]


      #check="/article/"
      #check1="/opinion/"
      for c in catagories:
        url="https://www.nytimes.com/search?dropmab=false&endDate="+year+month+str(day)+"&query=&sections="+c+"%7Cnyt%3A%2F%2Fsection%2F70e865b6-cc70-5181-84c9-8368b3a5c34b&sort=best&startDate="+year+month+str(day)+"&types=article"
        print(">>>>>", url)
        reqs = requests.get(url)
        soup = BeautifulSoup(reqs.text, 'html.parser')
        
        for link in soup.find_all("a"):
          url=str(link.get('href'))
          if ".html" in url:
            vaildation=0
            for a in areas:
              if a in url:
                vaildation=1
            if  url not in self.articles and vaildation==1: 
              if "https://www.nytimes.com/" not in url and url[0]=='/':
                url="https://www.nytimes.com/"+url   
              self.articles.append(url)
              print(url)    
  def getarticle(self, x, d, m, year):
    print(" => downloading..... "+x)
    flage=1
    try:
      toi_article = Article(x, language="en") # en for English
      toi_article.download()
      toi_article.parse()
    except newspaper.article.ArticleException:
      flage=0
    
    if flage==1:
      #To extract title
      #print("Article's Title:")
      #print(toi_article.title)
      #print("n")
      #To extract text
      #print("Article's Text:")
      #print(toi_article.text)
      #print("n")
      #print(toi_article.publish_date)
      link=x
      title=str(toi_article.title)
      author=str(toi_article.authors)
      article=str(toi_article.text)
      source="The New York Times"
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(year)  
      month=months[m-1]
      DateOfPublication=year+"-"+month+"-"+day
      DateOfExtraction=str(datetime.date(datetime.now()))
      #print(source)
      #print(DateOfExtraction)
      #print(DateOfPublication)
      #print(title)
      #print(author)
      #print(article)
      return source, link, DateOfExtraction, DateOfPublication, title, author, article
    else:
      return "0","0","0","0","0","0" ,"0"  
  def extract(self, date1, date2, c):
    from datetime import datetime
    Datetoday=str(datetime.date(datetime.now()))
    print("\n => scraper: Dawn news")



    if c==0: #defined range
      datei=date1.split('-',2)
      year=int(datei[0])
      m1=int(datei[1])
      d1=int(datei[2])
      datef=date2.split('-',2)
      year=int(datef[0])
      m2=int(datef[1])
      d2=int(datef[2])

      url=""
      chk=0
      for j in range(m1, m2+1):
          df=31
          di=1
          if j==m1:
              di=d1
          if j==m2:
              df=d2
          for i in range(di, df+1):
              self.articles.clear()
              self.downloaddata(i, j, year, url, chk)

"""### Scrapper: Dawn News"""

class Dawn(NewsExtractor):

  def getdatelinks(self, d, m, y):
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(y)  
      month=months[m-1]
      print(" => getting article links...........", day,month+",",year)
      import requests
      from bs4 import BeautifulSoup
      catagories=["1","2"]

      #check="/article/"
      #check1="/opinion/"
      for c in catagories:
        url="https://www.dawn.com/archive/"+year+"-"+month+"-"+str(day)
        print(">>>>>", url)
        reqs = requests.get(url)
        soup = BeautifulSoup(reqs.text, 'html.parser')
        
        for link in soup.find_all("a", {"class": "story__link"}):
        
            url=word=str(link.get('href'))
            if  url not in self.articles:    
              self.articles.append(url)
              print(url)    
  def getarticle(self, x, d, m, year):
    print(" => downloading..... "+x)
    flage=1
    try:
      toi_article = Article(x, language="en") # en for English
      toi_article.download()
      toi_article.parse()
    except newspaper.article.ArticleException:
      flage=0
    
    if flage==1:
      #To extract title
      #print("Article's Title:")
      #print(toi_article.title)
      #print("n")
      #To extract text
      #print("Article's Text:")
      #print(toi_article.text)
      #print("n")
      #print(toi_article.publish_date)
      link=x
      title=str(toi_article.title)
      author=str(toi_article.authors)
      article=str(toi_article.text)
      source="Dawn"
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(year)  
      month=months[m-1]
      DateOfPublication=year+"-"+month+"-"+day
      DateOfExtraction=str(datetime.date(datetime.now()))
      #print(source)
      #print(DateOfExtraction)
      #print(DateOfPublication)
      #print(title)
      #print(author)
      #print(article)
      return source, link, DateOfExtraction, DateOfPublication, title, author, article
    else:
      return "0","0","0","0","0","0" ,"0"  
  def extract(self, date1, date2, c):
    from datetime import datetime
    Datetoday=str(datetime.date(datetime.now()))
    print("\n => scraper: Dawn news")



    if c==0: #defined range
      datei=date1.split('-',2)
      year=int(datei[0])
      m1=int(datei[1])
      d1=int(datei[2])
      datef=date2.split('-',2)
      year=int(datef[0])
      m2=int(datef[1])
      d2=int(datef[2])

      url=""
      chk=0
      for j in range(m1, m2+1):
          df=31
          di=1
          if j==m1:
              di=d1
          if j==m2:
              df=d2
          for i in range(di, df+1):
              self.articles.clear()
              self.downloaddata(i, j, year, url, chk)

"""### Scrapper: Tribune News"""

class tribune(NewsExtractor):

  def getdatelinks(self, d, m, y):
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(y)  
      month=months[m-1]
      print(" => getting article links...........", day,month+",",year)
      import requests
      from bs4 import BeautifulSoup
      catagories=["1"]

      #check="/article/"
      #check1="/opinion/"
      for c in catagories:
        
        url="https://tribune.com.pk/listing/web-archive/"+year+"-"+month+"-"+str(day)
        print(">>>>>", url)
        reqs = requests.get(url)
        soup = BeautifulSoup(reqs.text, 'html.parser')
        
        for link in soup.find_all("a"):
          
          url=str(link.get('href'))
          if "/story/" in url:
            if  url not in self.articles:    
              self.articles.append(url)
              #print(url)    
  def getarticle(self, x, d, m, year):
    print(" => downloading..... "+x)
    flage=1
    try:
      toi_article = Article(x, language="en") # en for English
      toi_article.download()
      toi_article.parse()
    except newspaper.article.ArticleException:
      flage=0
    
    if flage==1:
      #To extract title
      #print("Article's Title:")
      #print(toi_article.title)
      #print("n")
      #To extract text
      #print("Article's Text:")
      #print(toi_article.text)
      #print("n")
      #print(toi_article.publish_date)
      link=x
      title=str(toi_article.title)
      author=str(toi_article.authors)
      article=str(toi_article.text)
      source="Tribune"
      months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] 
      if d<10:
        day="0"+str(d)
      else:
        day=str(d)
      year=str(year)  
      month=months[m-1]
      DateOfPublication=year+"-"+month+"-"+day
      DateOfExtraction=str(datetime.date(datetime.now()))
      #print(source)
      #print(DateOfExtraction)
      #print(DateOfPublication)
      #print(title)
      #print(author)
      #print(article)
      return source, link, DateOfExtraction, DateOfPublication, title, author, article
    else:
      return "0","0","0","0","0","0" ,"0"  
  def extract(self, date1, date2, c):
    from datetime import datetime
    Datetoday=str(datetime.date(datetime.now()))
    print("\n => scraper: Dawn news")



    if c==0: #defined range
      datei=date1.split('-',2)
      year=int(datei[0])
      m1=int(datei[1])
      d1=int(datei[2])
      datef=date2.split('-',2)
      year=int(datef[0])
      m2=int(datef[1])
      d2=int(datef[2])

      url=""
      chk=0
      for j in range(m1, m2+1):
          df=31
          di=1
          if j==m1:
              di=d1
          if j==m2:
              df=d2
          for i in range(di, df+1):
              self.articles.clear()
              self.downloaddata(i, j, year, url, chk)

"""### Scrapper Object Initilization & Directory Path Set"""

o1=TOI()
o1.setdirpath("/content/drive/MyDrive/datasets/toi/")
o2=NYT()
o2.setdirpath("/content/drive/MyDrive/datasets/NYT/")
o3=Dawn()
o3.setdirpath("/content/drive/MyDrive/datasets/dawn/")
o4=tribune()
o4.setdirpath("/content/drive/MyDrive/datasets/tribune/")

"""### Scrapper Exectuation Daily"""

o1.daily()
o2.daily()
o3.daily()
o4.daily()

"""### Scrapper Exectuation In Date Range """

#date1: FROM date to start news etraction
#date2: TO date to end news etraction
date1="2021-07-26"    
date2="2021-07-26"

#Call parser objects in date-range
o1.rangedate(date1, date2)
o2.rangedate(date1, date2)
o3.rangedate(date1, date2)
o4.rangedate(date1, date2)

"""### Scrapper Exectuation Weekly"""

o1.weekly()
o2.weekly()
o3.weekly()
o4.weekly()

"""### Scrapper Exectuation Monthly"""

o1.monthly()
o2.monthly()
o3.monthly()
o4.monthly()